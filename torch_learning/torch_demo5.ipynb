{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Neural Network ，导入构建神经网络必要的库\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get Device for Training 获取训练设备 CPU or GPU\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Class  定义神经网络类\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()#将输入的图像展平为一维向量\n",
    "        # 定义一个包含三个全连接层的神经网络\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),#第一个，将输入的784个像素点转换为512个像素点\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),#第二个，512个像素点不变\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),#第三个，将512个像素点转换为10个像素点，对应数据集里的10个类别\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)#利用归一化指数函数将输出转换为概率\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 以 3 幅大小为 28x28 的图像为样本\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# 对 nn.Flatten 层进行初始化，将每幅 28x28 的二维图像转换成一个包含 784 个像素值的连续数组（保持最小批次维度（dim=0））\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 线性层是一个利用其存储的权重和偏置对输入进行线性变换的模块\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.1731, -0.5297,  0.4760, -0.1307, -0.6925,  0.1529,  0.2574,  0.4343,\n",
      "          0.4245, -0.1047, -0.1392,  0.2974, -0.4514,  0.5146, -0.5924,  0.2685,\n",
      "         -0.3137, -0.0582,  0.6242,  0.3661],\n",
      "        [-0.6046,  0.2460,  0.2023,  0.2056, -0.1582, -0.1251, -0.1164,  0.5647,\n",
      "          0.6052, -0.3545, -0.0267,  0.2248, -0.6838,  0.3021, -0.6478,  0.2053,\n",
      "         -0.3345, -0.1805,  0.0174,  0.1722],\n",
      "        [-0.2132, -0.2045,  0.2342, -0.1881, -0.4129, -0.4041,  0.0410,  0.4493,\n",
      "          0.5612, -0.0177, -0.0012, -0.0074, -0.4159,  0.3742, -0.6163,  0.2103,\n",
      "         -0.2913, -0.3911,  0.2648,  0.2478]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.4760, 0.0000, 0.0000, 0.1529, 0.2574, 0.4343, 0.4245,\n",
      "         0.0000, 0.0000, 0.2974, 0.0000, 0.5146, 0.0000, 0.2685, 0.0000, 0.0000,\n",
      "         0.6242, 0.3661],\n",
      "        [0.0000, 0.2460, 0.2023, 0.2056, 0.0000, 0.0000, 0.0000, 0.5647, 0.6052,\n",
      "         0.0000, 0.0000, 0.2248, 0.0000, 0.3021, 0.0000, 0.2053, 0.0000, 0.0000,\n",
      "         0.0174, 0.1722],\n",
      "        [0.0000, 0.0000, 0.2342, 0.0000, 0.0000, 0.0000, 0.0410, 0.4493, 0.5612,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.3742, 0.0000, 0.2103, 0.0000, 0.0000,\n",
      "         0.2648, 0.2478]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#使用ReLU激活函数\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")#打印激活前的张量值\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")#ReLU激活函数将所有负值转换为零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential 是一个有序的模块，数据按照定义的相同顺序通过模块\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络的最后一个线性层返回的是logits，这些值随后被传递给nn.Softmax，logits被缩放到[0, 1]范围内的值，表示模型对每个类别的预测概率。dim参数指定了值必须相加为1的维度。\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0176,  0.0335,  0.0140,  ..., -0.0224, -0.0296,  0.0105],\n",
      "        [ 0.0240, -0.0333,  0.0353,  ...,  0.0161,  0.0101,  0.0275]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0099,  0.0019], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0216,  0.0281,  0.0399,  ...,  0.0240,  0.0315,  0.0204],\n",
      "        [ 0.0190, -0.0353, -0.0260,  ..., -0.0123, -0.0288, -0.0208]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0255, 0.0023], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0147, -0.0397,  0.0365,  ...,  0.0029,  0.0274, -0.0069],\n",
      "        [-0.0315, -0.0094, -0.0165,  ...,  0.0319,  0.0370, -0.0046]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0076,  0.0338], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nn.Module 会自动跟踪模型对象内定义的所有字段，并使用模型的 parameters() 或 named_parameters() 方法访问所有参数\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "#遍历模型的所有参数\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wintorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
